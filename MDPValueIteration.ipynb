{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Ian McGregor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372b2e64a091bc5c1e72fcc5f697fe12",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2021:  Assignment 5  \n",
    "\n",
    "Shortcuts:  [top](#top) -- [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) -- [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c31566edfa1fdd0af96343452430d5f3",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment Overview\n",
    "\n",
    "This assignment is an exercise in implementing and analyzing Markov Decision Processes (MDPs). Problem 1 asks you to code a solution to a specific scenario, while Problem 2 is a conceptual question which asks you to describe an MDP problem of your own design.\n",
    "\n",
    "Here's a summary of the tasks required and the associated points:\n",
    "\n",
    "| Problem #  | Tasks                                                  | Points  |\n",
    "|:---        |:---                                                    |:---:    |\n",
    "| 1a         | Code: Complete implementation of `MDP` class           | 10      |\n",
    "| 1b         | Code: Implement `value_iteration` and `find_policy`    | 5       |\n",
    "| 1c         | Code and create: Generate and illustrate optimal path  | 5       |\n",
    "| 1d         | Written: analyze policy                                | 5       |\n",
    "| 1e         | Code: adjust non-terminal rewards                      | 5       |\n",
    "| 1f         | Code and write: adjust terminal rewards                | 5       |\n",
    "| 1g         | Written: analyze transition model                      | 5       |\n",
    "| 2a         | Written: define problem                                | 4       |\n",
    "| 2b         | Written: define states                                 | 4       |\n",
    "| 2c         | Written: define reward                                 | 4       |\n",
    "| 2d         | Written: define actions and transition                 | 4       |\n",
    "| 2e         | Written: define optimal policy                         | 4       |\n",
    "| Total      |                                                        | 60      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "# add any imports you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c608e4fde84b1104aa33382b2642da7",
     "grade": false,
     "grade_id": "1-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 1: Navigating an awkward situation with grace and poise\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/5/5f/interaction.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Suppose you are at a social event where you would like to avoid any interaction with a large number of the other attendees. It's not that you don't like them, it's just that you don't like *talking to* them. A few of your good friends are also in attendance, but they are tucked away in a corner. The rectangular room in which the event is being held spans gridcells at $x=1,2,\\ldots, 6$ and $y=1,2,\\ldots, 5$. At the eastern edge ($x=6$) of this first floor room, there is a balcony, with a 6-foot drop. If the event becomes unbearably awkward, you can jump off the balcony and run away. Of course, this might hurt a little bit, so we should incorporate this into our reward structure.\n",
    "\n",
    "The terminal states and rewards associated with them are given in the diagram below. The states are represented as $(x,y)$ tuples. The available actions in non-terminal states include moving exactly 1 unit North (+y), South (-y), East (+x) or West (-x), although you should not include walking into walls, because that would be embarrassing in front of all these other people. Represent actions as one of 'N', 'S', 'E', or 'W'. For now, assume all non-terminal states have a default reward of -0.01, and use a discount factor of 0.99.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the following transition model for this decision process, if you are trying to move from state $s$ to state $s'$:\n",
    "* you successfully move from $s$ to $s'$ with probability 0.6\n",
    "* the remaining 0.4 probability is spread equally likely across state $s$ **and** all adjacent (N/S/E/W) states except for $s'$. Note that this does not necessarily mean that all adjacent states have 0.1, because some states do not have 4 adjacent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "823e20cadbdae74fd72ea6227b26eb1c",
     "grade": false,
     "grade_id": "1a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1a'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1a) - 10 points\n",
    "\n",
    "Complete the `MDP` class below. The docstring comments provide some desired specifications. You may add additional methods or attributes, if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e28782999a0a5c35e0461daeb267be5c",
     "grade": false,
     "grade_id": "mdp-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal_states, default_reward, df):\n",
    "        \n",
    "        # self.states -- a list of all the states as (x,y) tuples\n",
    "        self.states = list()\n",
    "        if not self.states:\n",
    "            for i in range(ncol):\n",
    "                for j in range(nrow):\n",
    "                    self.states.append((i+1,j+1))\n",
    "        # self.terminal_states -- a dictionary with terminal state keys, and rewards as values\n",
    "        self.terminal_states = terminal_states\n",
    "        # self.default_reward -- the reward for being in any non-terminal state\n",
    "        self.default_reward = default_reward\n",
    "        # self.df -- discount factor\n",
    "        self.df = df\n",
    "        self.maxrow = nrow\n",
    "        self.maxcol = ncol\n",
    "        self.path = list()\n",
    "\n",
    "    def actions(self, state):\n",
    "        '''Return a list of available actions from the given state.\n",
    "        Possible actions are 'N','S','E','W'\n",
    "        [None] are the actions available from a terminal state.\n",
    "        '''\n",
    "        if(state in self.terminal_states):\n",
    "            return [None]\n",
    "        else:\n",
    "            x, y = state\n",
    "            actions = ['N', 'S', 'E', 'W']\n",
    "            if(x == 1):\n",
    "                actions.remove('W')\n",
    "            if(y == 1):\n",
    "                actions.remove('S')\n",
    "            if(x == self.maxcol):\n",
    "                actions.remove('E')\n",
    "            if(y == self.maxrow):\n",
    "                actions.remove('N')\n",
    "            return actions\n",
    "        \n",
    "    def reward(self, state):\n",
    "        '''Return the reward for being in the given state'''\n",
    "        if(state in self.terminal_states):\n",
    "            return self.terminal_states[state]\n",
    "        else:\n",
    "            return self.default_reward\n",
    "        \n",
    "    def result(self, state, action):\n",
    "        '''Return the resulting state (as a tuple) from doing the given\n",
    "        action in the given state, without uncertainty. Uncertainty\n",
    "        is incorporated into the transition method.\n",
    "        state -- a tuple representing the current state\n",
    "        action -- one of N, S, E or W, as a string\n",
    "        '''\n",
    "        x, y = state\n",
    "        if(action == 'Stay'):\n",
    "            return(x,y)\n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        \n",
    "        if(action == 'N'):\n",
    "            y = y + 1\n",
    "            return (x,y)\n",
    "        elif(action == 'S'):\n",
    "            y = y - 1\n",
    "            return (x,y)\n",
    "        elif(action == 'E'):\n",
    "            x = x + 1\n",
    "            return (x,y)\n",
    "        elif(action == 'W'):\n",
    "            x = x - 1\n",
    "            return (x,y)\n",
    "\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        '''Return a list of (probability, next_state) associated\n",
    "        with the possibilities of taking the given action from the given state.\n",
    "        '''\n",
    "        if action is None:\n",
    "            # This happens for a terminal state\n",
    "            return [(0, state)]\n",
    "        else:\n",
    "            # Make a list of actions available\n",
    "            actionList = self.actions(state)\n",
    "            actionList.append('Stay')\n",
    "            # Make an empty list to contain future states\n",
    "            futureStates = []\n",
    "            for i in range(len(actionList)):\n",
    "                # If action in list is equal to desired action, add resulting state with probability 0.6 to future state list\n",
    "                if (actionList[i] == action):\n",
    "                    futureStates.append((0.6, self.result(state, actionList[i])))\n",
    "                # Not the desired action\n",
    "                else:\n",
    "                    futureStates.append((0.4/(len(actionList) -1), self.result(state, actionList[i])))\n",
    "            return futureStates\n",
    "\n",
    "    def expected_utility(self, next_states, cur_util):\n",
    "        '''Return the expected utility given generated list of possible \n",
    "        next states and the current utility, which is a dictionary of the form {state : utility}\n",
    "        '''\n",
    "        sum = 0\n",
    "        \n",
    "        for i in range(len(next_states)):\n",
    "            probability, state = next_states[i]\n",
    "            sum = sum + (probability * cur_util[state])\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75dc0b42cb21d9584de2037444b26d51",
     "grade": false,
     "grade_id": "1a-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1a) tests\n",
    "\n",
    "Note that these are non-exhaustive, because there is some flexibility in how the `transition` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e36e4589b1131c2c413d7a804bf7f4c",
     "grade": true,
     "grade_id": "1a-test-simple",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "nrow = 3\n",
    "ncol = 3\n",
    "default_reward = -0.2\n",
    "discount = 0.5\n",
    "terminal = {(1,3):-1, (1,2):2}\n",
    "mdp_simple = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "actions1 = set(mdp_simple.actions((2,2)))\n",
    "assert (actions1 == {'N','S','E','W'}), \"Expected set of actions is {'N','S','E','W'}, your code returned: %s\" % actions1\n",
    "\n",
    "actions2 = set(mdp_simple.actions((1,1)))\n",
    "assert (actions2 == {'N','E'}), \"Expected set of actions is {'N','E'}, your code returned: %s\" % actions2\n",
    "\n",
    "actions3 = set(mdp_simple.actions((1,2)))\n",
    "assert (actions3 == {None}), \"Expected set of actions is {None}, your code returned: %s\" % actions3\n",
    "\n",
    "reward1 = mdp_simple.reward((1,2))\n",
    "assert (reward1 == 2), \"Expected reward is 2, your code returned: %f\" % reward1\n",
    "\n",
    "reward2 = mdp_simple.reward((2,2))\n",
    "assert (reward2 == -0.2), \"Expected reward is -0.2, your code returned: %f\" % reward2\n",
    "\n",
    "result1 = mdp_simple.result((1,1), 'N')\n",
    "assert (result1 == (1,2)), \"Expected result is (1,2), your code returned: %s\" % (result1,)\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6b6124959228ec56ece931028694a1",
     "grade": true,
     "grade_id": "1a-test-problem",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "# set values from problem statement\n",
    "nrow = 5\n",
    "ncol = 6\n",
    "default_reward = -0.01\n",
    "discount = 0.99\n",
    "terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "            (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "mdp_p1 = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "# Find the expected utility of walking N from (1,1):\n",
    "util_old = {s : s[0]+s[1] for s in mdp_p1.states}\n",
    "\n",
    "next_states = mdp_p1.transition((1,1), 'N')\n",
    "assert (len(next_states) == 3), \"Expected 3 possible next states, your code returned: %d\" % len(next_states)\n",
    "\n",
    "exp_util = mdp_p1.expected_utility(next_states, util_old)\n",
    "assert (exp_util == 2.8), \"Expected utility should be 2.8, your code returned %f\" % exp_util\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1171844cdb0baf1260559a6f58a18477",
     "grade": false,
     "grade_id": "1b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1b'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1b) - 5 points\n",
    "\n",
    "Implement value iteration to calculate the utilities for each state.  Also implement a function that takes as arguments an `MDP` object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy.  The optimal policy dictionary should have state tuples as keys and the optimal move (None, N, S, E or W) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c8c83a0cd970ca44ba24d8f8715781",
     "grade": false,
     "grade_id": "value-iter-find-policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, tol=1e-3):\n",
    "    u = {}\n",
    "    uPrime = {}\n",
    "    gamma = 0.99\n",
    "    # 1. initialize utility for all states to 0 -> U\n",
    "    for n in range(len(mdp.states)):\n",
    "        uPrime[mdp.states[n]] = 0\n",
    "        u[mdp.states[n]] = 0\n",
    "    # Repeat 2 until problem is converged\n",
    "    counter = 0\n",
    "    while(True):\n",
    "        delta = 0\n",
    "        for a in range(len(u)):\n",
    "            u[mdp.states[a]] = uPrime[mdp.states[a]]\n",
    "        # 2. for each state on the board\n",
    "        for i in range(len(mdp.states)):\n",
    "            #    2.1. calculate expected utility for each possible next state\n",
    "            # List of actions for current state\n",
    "            actions = mdp.actions(mdp.states[i])\n",
    "            # R(s)\n",
    "            reward = mdp.reward(mdp.states[i])\n",
    "            # List of utility scores\n",
    "            futureUtil = []\n",
    "            for j in range(len(actions)):\n",
    "                next_states = mdp.transition(mdp.states[i], actions[j])\n",
    "                util = mdp.expected_utility(next_states, u)\n",
    "                futureUtil.append(util)\n",
    "                #    2.2. find best utility out of possible expected utilities\n",
    "                #    2.3. define new utility of the state\n",
    "            uPrime[mdp.states[i]] = (max(futureUtil)*gamma) + reward\n",
    "            # update delta\n",
    "            if((abs(uPrime[mdp.states[i]] - u[mdp.states[i]]) > delta)):\n",
    "                delta = abs(uPrime[mdp.states[i]] - u[mdp.states[i]])\n",
    "        if(delta < tol * ((1-gamma)/gamma)):\n",
    "            return u\n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    '''Return a dictionary containing the best policy for each state s,\n",
    "    of the form {s : s_policy}\n",
    "    '''\n",
    "    pi = {}\n",
    "    # 1. initialize utility for all states to '' -> U\n",
    "    for n in range(len(mdp.states)):\n",
    "        pi[mdp.states[n]] = ''\n",
    "    # For each state on the board\n",
    "    for i in range(len(mdp.states)):\n",
    "        #    Calculate optimal policy at each state\n",
    "        #    List of actions for current state\n",
    "        actions = mdp.actions(mdp.states[i])\n",
    "        curUtil = -5\n",
    "        bestAction = ''\n",
    "        for k in range(len(actions)):\n",
    "            if(actions[k] != None and curUtil < utility[mdp.result(mdp.states[i], actions[k])]):\n",
    "                curUtil = utility[mdp.result(mdp.states[i], actions[k])]\n",
    "                # curUtil = sum([p*utility[s2] for p, s2 in mdp.transition(mdp.states[i],actions[k])])\n",
    "                bestAction = actions[k]\n",
    "        pi[mdp.states[i]] = bestAction\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f14e15e8c0ad815923f021a8797e81e",
     "grade": false,
     "grade_id": "1b-tests-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1b) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54d20c4e3b584c7e7863d35660db1f6c",
     "grade": true,
     "grade_id": "1b-tests-asserts",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed test cases: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "\n",
    "utility = value_iteration(mdp_p1, tol=1e-6)\n",
    "policy = find_policy(mdp_p1, utility)\n",
    "\n",
    "util1 = utility[(1,5)]\n",
    "assert (util1 == 2), \"Expected utility of 2, your code returned %f\" % util1\n",
    "\n",
    "util2 = utility[(6,1)]\n",
    "assert (util2 == -5), \"Expected utility of -5, your code returned %f\" % util2\n",
    "\n",
    "util3 = round(utility[(2,5)],2)\n",
    "assert (util3 == 1.74), \"Expected utility of 1.74, your code returned %f\" % util3\n",
    "\n",
    "util4 = round(utility[(5,3)],2)\n",
    "assert (util4 == -1.39), \"Expected utility of -1.39, your code returned %f\" % util4\n",
    "\n",
    "policy1 = policy[(2,4)]\n",
    "assert (policy1 == 'W'), \"Expected policy is 'W', your code returned %s\" % policy1\n",
    "\n",
    "policy2 = policy[(1,1)]\n",
    "assert (policy2 == 'N'), \"Expected policy is 'N', your code returned %s\" % policy2\n",
    "\n",
    "print(\"Passed test cases: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b6b83bb256c438e69fc7bb280e5a80",
     "grade": false,
     "grade_id": "1c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1c'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1c) - 5 points\n",
    "\n",
    "If we enter the room at $s_0$, what is the optimal path for us to follow? Complete the following function to generate the sequence of states along the path. If we start in state $s_0$, then your output should be in the form $[s_0, s_1, s_2, ... , s_{term}]$ where $s_{term}$ is a terminal state. Set your tolerance for value iteration to be $10^{-6}$\n",
    "\n",
    "Additionally, create a graphic to illustrate this policy pathway, either by generating a plot in Python or by uploading a hand-drawn image and including a link to it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740f51d6586bfb8d597b3afe8e46b108",
     "grade": false,
     "grade_id": "1c-answer-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_path(mdp, state):\n",
    "\n",
    "    utility = value_iteration(mdp, tol=1e-6)\n",
    "    policy = find_policy(mdp, utility)\n",
    "    next_state = mdp.result(state, policy[state])\n",
    "    path = list()\n",
    "    path.append(state)\n",
    "    while(True):\n",
    "        path.append(next_state)\n",
    "        next_state = mdp.result(next_state, policy[next_state])\n",
    "        if(next_state in mdp.terminal_states):\n",
    "            path.append(next_state)\n",
    "            return path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "085e7a8e272b36a76e8ce82e1537152e",
     "grade": false,
     "grade_id": "1c-answer-graphic",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Put a link to your graphic below for the path from (5,3). You can find an [example image here](http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp_path.png) with the optimal path starting from (5,1). **Please include a link rather than attaching a file**. This can be a link to your file in Google Drive, with the permissions set to public. The graders will not ask for permissions! The syntax for including a link in markdown is `[link text](url.com/to/your/image)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd08159d2a99ee23492b15403f5e025",
     "grade": true,
     "grade_id": "1c-graphic",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "[Masterpiece](https://drive.google.com/file/d/1kP5M4KjHIowt_7-Lgy2tiCS-rKO-uk6b/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f5c9f5250a8cb8db949d1166219cf06",
     "grade": true,
     "grade_id": "1c-tests-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Passed tests: 3 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "assert (path1 == [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path1\n",
    "\n",
    "path2 = find_optimal_path(mdp_p1, (5,1))\n",
    "assert (path2 == [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path2\n",
    "\n",
    "path3 = find_optimal_path(mdp_p1, (1,1))\n",
    "assert (path3 == [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path3\n",
    "\n",
    "print(\"Passed tests: 3 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b468abfa8603618ab21ea57d3a26d564",
     "grade": false,
     "grade_id": "1d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1d'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1d) - 5 points\n",
    "\n",
    "From (3,2) the optimal move is to walk West. If we are trying to go talk to our friends in the Northwest corner, why would we rather do this than walk North first, then West?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75467f69d87838440cb5b2a3dd342a34",
     "grade": true,
     "grade_id": "1d-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "If we were to move north to (3,3) we would be neighboring 2 people whom we don't exactly want to talk to, which leaves room for error in movement. If there is an error in movement, then we may end up accidentally talking to them, which is precisely what we are trying to avoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "745fc8b0c5373d58b894072f14d2bcf8",
     "grade": false,
     "grade_id": "1e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1e'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1e) - 5 points\n",
    "\n",
    "How painfully awkward do you need to set the default reward for non-terminal states before the optimal move at (5,1) becomes jumping off the balcony immediately and running away? Implement the following function which returns the reward where the policy for (5,1) is to jump off the balcony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a84df7c9004254b67cada546239f68",
     "grade": false,
     "grade_id": "1e-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_non_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    discount = 0.99\n",
    "    terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "                (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "    for reward in np.arange(-0.01, -3, -0.01):\n",
    "        mdp = MDP(nrow, ncol, terminal, reward, discount)\n",
    "        utility = value_iteration(mdp, tol=1e-3)\n",
    "        policy = find_policy(mdp, utility)\n",
    "        if (policy[5,1] == ''):\n",
    "            return reward  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4346aaa5a01e06b338b3482b225aaa30",
     "grade": true,
     "grade_id": "1e-tests-assert",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = find_non_terminal_reward()\n",
    "assert (reward1 == -2.09), \"The expected reward is -2.09, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Test passed: 5 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e194e2f330afbd4fe1437648f6f86723",
     "grade": false,
     "grade_id": "1f-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1f'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1f) - 5 points\n",
    "\n",
    "In **1e** we assumed a certain level of loss (negative reward) just for being present.  But a more realistic approach might be to instead change the reward structure for the terminal states. Consider the terminal states with -1 reward in the default model. Let $R^*$ denote the reward associated with these states. How low does $R^*$ need to be in order for us to immediately jump off the balcony and run away? Use the default non-terminal state reward of -0.01. Implement the following function to return the value of $R^*$ which leads to a policy of jumping off the balcony at (5,1). Write a few sentences interpreting your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03cd66008c7b709512677ede48059600",
     "grade": false,
     "grade_id": "1f-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_terminal_reward():\n",
    "    nrow = 5\n",
    "    ncol = 6\n",
    "    discount = 0.99\n",
    "    default_reward = -0.01\n",
    "    for rstar in np.arange(-6, -12, -0.01):\n",
    "        # 1. set the reward of the terminal nodes \n",
    "        terminal = {(1,3):rstar, (1,4):2, (1,5):2, (2,1):rstar, (3,1):rstar, (3,4):rstar, (3,5):1,\n",
    "                (4,3):rstar, (4,4):rstar, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "        # 2. define MDP with appropriate parameters\n",
    "        mdp = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "        utility = value_iteration(mdp, tol=1e-3)\n",
    "        policy = find_policy(mdp, utility)\n",
    "        # 3. find policy for state (5,1)\n",
    "        if (policy[5,1] == ''):\n",
    "            # 4. return R* if the policy for (5,1) is to jump off the balcony\n",
    "            pprint.pprint(utility)\n",
    "            return rstar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7008fb02d7df7ba16c7882585e9b890d",
     "grade": false,
     "grade_id": "1f-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1f) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5230fc87068ca9643b6d7333441b715",
     "grade": true,
     "grade_id": "1f-test-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{(1, 1): -6.833459774489048,\n (1, 2): -5.41283352156081,\n (1, 3): -11.389999999999885,\n (1, 4): 2.0,\n (1, 5): 2.0,\n (2, 1): -11.389999999999885,\n (2, 2): -3.8431806854447985,\n (2, 3): -2.1921451600621142,\n (2, 4): -0.019436479414799,\n (2, 5): 1.5062608910461857,\n (3, 1): -11.389999999999885,\n (3, 2): -4.92015112919688,\n (3, 3): -4.49994054313204,\n (3, 4): -11.389999999999885,\n (3, 5): 1.0,\n (4, 1): -6.121672307468489,\n (4, 2): -5.7283760765497105,\n (4, 3): -11.389999999999885,\n (4, 4): -11.389999999999885,\n (4, 5): -1.4137848707730587,\n (5, 1): -5.124568310657509,\n (5, 2): -5.000516701460532,\n (5, 3): -4.925974296181743,\n (5, 4): -3.8899720740860646,\n (5, 5): -2.3309495896804235,\n (6, 1): -5.0,\n (6, 2): -5.0,\n (6, 3): -5.0,\n (6, 4): -5.0,\n (6, 5): -5.0}\nPassed test: 3 points\n"
     ]
    }
   ],
   "source": [
    "reward1 = round(find_terminal_reward(),2)\n",
    "assert (reward1 == -11.39), \"Expected reward is -11.39, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Passed test: 3 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b7fd68f561b33ad94ed8a286a7eb3d6",
     "grade": false,
     "grade_id": "1f-reflection-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a few sentences with your interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e68d9c45421d6fd097011fd0b9f8bd",
     "grade": true,
     "grade_id": "1f-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I have output the utility using pretty print for reference. As you can see if we are starting at position (5,1) the utilities for the surrounding states are: <br>\n",
    "    -- (5, 2): -5.000516701460532 <br>\n",
    "    -- (4, 1): -6.121672307468489 <br>\n",
    "    and finally <br>\n",
    "    -- (6, 1): -5.0 <br>\n",
    "    The optimal policy would be to throw yourself off the balcony as the rest of the room must be either: filled with bees or literally on fire. <br>\n",
    "    No but seriously, as the pitfalls in the room become increasingly consequential, our value iteration, and policy iteration algorithms compute that the risk of falling into any of these pitfalls accidentally is far more dangerous than jumping off the balcony. Some examples that come to mind could be as I stated before, maybe a couple of killer hornet nests are in the room, or DEFINITELY COVID POSISTIVE people without masks are literally coughing incessantly in the room. Whatever the case may be, as we iterate over all possible states and actions the correct decision becomes effortless and clear, talking to your friends is not worth dying for. This concludes my TedTalk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2e45eceb59f1d36e0c528cd7faf22b",
     "grade": false,
     "grade_id": "1g-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1g'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1g) - 5 points\n",
    "\n",
    "Given the problem context, write a few sentences about why this is or is not an appropriate transition model. Include an interpretation of the terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a884715306f6da7b8d2b7fd566f7732e",
     "grade": true,
     "grade_id": "1g-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I think the transition makes perfect sense. In the context of socializing at a party, I would find myself weighing these exact same decisions. I'm pretty particular about whom I talk to, and I am strongly averse to engaging in small talk. (Sure waste the only true non-renewable resource by talking about the weather - time) <br>\n",
    "The terminal states that are not jumping off of the balcony are pretty accurate, while you're certainly not going to break your leg(-5), talking to boring people is pretty unpleasant (-1). I would say that acheiving the goal state, and egaging in real conversation is pretty pleasurable, and in the context of a large party, very enticing (+2). If we are measuring the rewards at all terminal states by their relativity to each other, I would say that talking to my friends is not a +5, that is it is not the opposite of breaking my leg (-5) but +2 would be a reasonable measurement, as well as a -1 for egaging in boring conversation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5038f4053461860ec0728cc4626f059b",
     "grade": false,
     "grade_id": "2-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<a id='p2'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 2: Define your very own MDP\n",
    "\n",
    "For this problem, you do not need to write any code, but rather communicate your ideas clearly using complete sentences and descriptions of the concepts the questions ask about. You can, of course, include some pseudocode if it helps, but that is not strictly necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12e0af4a88a181762bd42ae609e93cf",
     "grade": false,
     "grade_id": "2a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2a'></a>\n",
    "\n",
    "## (2a) - 4 points\n",
    "\n",
    "Describe something you think would be interesting to model using a Markov decision process.  Be **creative** - do not use any examples from your homework, class, or the textbook, and if you are working with other students, please **come up with your own example**. There are so, SO many possible answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d13a848d94d3a2552c856f613b1fd2d",
     "grade": true,
     "grade_id": "2a-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "I was thinking of this question while I was grocery shopping last night and I realized, grocery shopping is absolutely perfect for an MDP model. Your living reward would be negative, and meausured in the amount of time you spend at each state. This example is of course limited to grocery shopping in a very specific context, lets say you have 5 minutes to find ben and jerrys ice cream for your very intoxicated and hangry friend, and they will only settle for Half Baked because they can't decide if they want chocolate vs vanilla ice cream, or cookie dough vs brownie batter. Naturally of course, Half Baked is the go-to, coming in second only to Boots on the Moon, which is my favorite flavor, but that is beside the point! ... Or is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33940776445caeaffd19100073cef2d0",
     "grade": false,
     "grade_id": "2b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2b'></a>\n",
    "\n",
    "## (2b) - 4 points\n",
    "\n",
    "What are the states associated with your MDP? Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "source": [
    "The states include the start state (entrance to the grocery store), the goal state (the freezer location that contains Half Baked), and the various pitfalls (Boots on the Moon of course, because your time is so limited, you don't have time to shop for ourselves), the chip aisle(again very distracting), and finally wherever they keep the bacon (duh)."
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c536e75b992639fa2b21877a388eda0f",
     "grade": true,
     "grade_id": "2b-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b9b36293b64ab2d3ba757666ac3427",
     "grade": false,
     "grade_id": "2c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2c'></a>\n",
    "\n",
    "## (2c) - 4 points\n",
    "\n",
    "What is the reward structure associated with your MDP?  Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b36dc1c0b6d151da8f42c1d93b9d9a",
     "grade": true,
     "grade_id": "2c-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The distracted states would end up in a terminal state with reward -5 and the location of Half Baked would have a whopping +10 reward. Of course the living reward is -.5 as time is precious and grocery stores are massive time sinks, and just plain massive. Discount of future rewards would 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "565e36ab6960052d86fa2a08c18f96c4",
     "grade": false,
     "grade_id": "2d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2d'></a>\n",
    "\n",
    "## (2d) - 4 points\n",
    "\n",
    "What are the actions and transition model associated with your MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b16a36bb225d610a1837822e543162f1",
     "grade": true,
     "grade_id": "2d-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The actions are move N,S,E,W and the transition model would depend on the various obstacles in your path such as other shoppers, aisles and various coolers placed directly in your path of acheiving the Half Baked extraction as fast as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ab2fc9b4eb20ce1b32e1a2aa0e1007",
     "grade": false,
     "grade_id": "2e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2e'></a>\n",
    "\n",
    "## (2e) - 4 points\n",
    "\n",
    "Interpret what an optimal policy represents in the context of your particular MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c57bad2508d2aa53541aaaa24b5f55c3",
     "grade": true,
     "grade_id": "2e-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The optimal policy would be to stay as far away from the distraction states, one of which is very close to your goal state, so we must ensure we enter the ice cream aisle from the opposite end of Boots on the Moon, as well as avoiding any obstacles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "519px",
    "left": "22px",
    "top": "149px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}